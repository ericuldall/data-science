{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#GA LA Data Science 7 - Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For this homework, we will be using the library tweepy to pull data from the Twitter API, \n",
    "and then do language detection on it. That is, you will build a classifier to predict what language a given tweet is, based only on its text.\n",
    "\n",
    "Start by installing tweepy with pip: ```sudo pip install tweepy```\n",
    "\n",
    "Be sure to consult the documentation at http://tweepy.readthedocs.org/en/v3.3.0/index.html \n",
    "read early, read often. **NOTE:** Tweepy is among the most popular Twitter+Python libraries and is recommended by Twitter. However, it lacks complete documentation, e.g. the documentation is missing the Stream class.\n",
    "\n",
    "For the early parts, you can also look at this blog post: \n",
    "http://adilmoujahid.com/posts/2014/07/twitter-analytics/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In order to access Twitter Streaming API, we need to get 4 pieces of information from Twitter: \n",
    "    API key, \n",
    "    API secret, \n",
    "    Access token, and \n",
    "    Access token secret. \n",
    "Follow the steps below to get all 4 elements:\n",
    "\n",
    "- Create a twitter account if you do not already have one.\n",
    "- Go to https://apps.twitter.com/ and log in with your twitter credentials.\n",
    "- Click \"Create New App\"\n",
    "- Fill out the form, agree to the terms, and click \"Create your Twitter application\"\n",
    "- In the next page, click on \"Keys and Access Tokens\" tab, and copy your \"API Key\" and \"API Secret\".\n",
    "- Scroll down and click \"Create my access token\", and copy your \"Access Token\" and \"Access Token Secret\".\n",
    "- Double-check and ensure you copied the entire tokens rather than just part of them.\n",
    "\n",
    "Now that that's all set up, let's get get on with the fun stuff!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['en' 'es' 'fr' 'zh' 'sv' 'de' 'und' 'ru' 'tl' 'pt' 'uk' 'it']\n"
     ]
    }
   ],
   "source": [
    "# To start, let's just get a basic listener set up. All we want to do here is see \n",
    "# a stream of data from Twitter. \n",
    "# If this works, you should see a bunch of json's in your output console. \n",
    "# (If you see anything else -- such as 3-digit numbers or nothing at all -- then it's time to debug.)\n",
    "import requests\n",
    "import random\n",
    "import time\n",
    "import urllib\n",
    "import hmac\n",
    "from hashlib import sha1\n",
    "import base64\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Variables that contains the user credentials to access Twitter API \n",
    "api_key = b\"yGrCEinj1esitqUKnluIVRGFH\"\n",
    "api_secret = b\"a4loFJuuob0Ct63O97xxD4vwFqsj5FCTdkEOZsQIYY90FgruPM\"\n",
    "\n",
    "\n",
    "'''\n",
    "auth_token = api_key + b':' + api_secret\n",
    "get_b_token = base64.b64encode(auth_token).decode()\n",
    "p = requests.post('https://api.twitter.com/oauth2/token', headers={'Authorization': 'Basic ' + get_b_token, 'Content-Type': 'application/x-www-form-urlencoded;charset=UTF-8'}, data='grant_type=client_credentials')\n",
    "'''\n",
    "bearer_token = 'AAAAAAAAAAAAAAAAAAAAANUxgwAAAAAA03BiCesaCR%2B1WqSd6e6i5oDfI8M%3DV5wVG3HWNovQTSjim1QZRWAHSkV2OYX4cux3n15rltfgYZldUh'\n",
    "\n",
    "\n",
    "search_url = \"https://api.twitter.com/1.1/search/tweets.json\"\n",
    "search_query = {\n",
    "        \"q\": \"#paris\",\n",
    "        \"count\": \"100\",\n",
    "        \"result_type\": \"mixed\"\n",
    "}\n",
    "\n",
    "langs = ['au', 'eu', 'en', 'es', 'mx', 'fr', 'zh', 'tw', 'sv', 'su', 'de', 'ro', 'ru', 'sc', 'sa', 'pt', 'uk', 'it']\n",
    "tags  = ['yolo', 'vive', 'us', 'earth', 'planet', 'math', 'lang', 'hund', 'dog', 'france', 'da', 'peace', 'love', 'family']\n",
    "\n",
    "df = []\n",
    "\n",
    "for lang in langs:\n",
    "    for tag in tags:\n",
    "        sq = search_query.copy()\n",
    "        sq['q'] = '#' + tag\n",
    "        sq['lang'] = lang\n",
    "        url = search_url + '?' + urllib.parse.urlencode(sq)\n",
    "\n",
    "        g = requests.get(url, headers={'Authorization': 'Bearer ' + bearer_token})\n",
    "        statuses = g.json().get('statuses')\n",
    "        try:\n",
    "            for status in statuses:\n",
    "                df.append({'TEXT': status.get('text'), 'LANG': status.get('lang')})\n",
    "        except:\n",
    "            print('whoops... not iterable!')\n",
    "    \n",
    "\n",
    "d = pd.DataFrame(df)\n",
    "print(d['LANG'].unique())\n",
    "d.to_csv('./tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now that that's working, let's filter our data. \n",
    "# This part is actually pretty easy. \n",
    "# Change the on_data method of ListenerParser to extract only the 'text' and 'language' fields, if present.\n",
    "# We also want to be able to retrieve a set number of results, so I have set a max_results parameter \n",
    "# in the constructor. Use that in your edit of on_data to make the object only retrieve up to \n",
    "# max_results many results. \n",
    "\n",
    "# This is a listener that will extract the data we are interested in and print to stdout\n",
    "# Note this class is the same class as in the last example. Except now, in on_data we\n",
    "#    are processing the tweets instead of just printing them!\n",
    "class ListenerParser(StreamListener):\n",
    "    \n",
    "    def __init__(self, max_results): \n",
    "        super(ListenerParser, self).__init__()\n",
    "        \n",
    "        self.texts = []   # List of tweet messages\n",
    "        self.langs = []   # List of tweet languages\n",
    "        \n",
    "        if max_results:\n",
    "            self.max_results = max_results\n",
    "        else: \n",
    "            self.max_results = float(\"inf\")\n",
    "    \n",
    "    def on_data(self, data):\n",
    "        if len(l.texts) >= self.max_results:\n",
    "            return False   # stop collecting data\n",
    "        try:\n",
    "            # Converts current tweet from JSON to Python data structures\n",
    "            data = json.loads(data)\n",
    "            \n",
    "            # Look inside the 'data' variable, or read the Twitter docs\n",
    "            text = 'YOUR ANSWER HERE'   # Text of the tweet (from 'data')\n",
    "            lang = 'YOUR ANSWER HERE'   # Language of the tweet (from 'data')\n",
    "            \n",
    "            # Print status every 500 tweets loaded\n",
    "            if not (len(l.texts) % 500): \n",
    "                print(str(len(l.texts)) + ' tweets loaded!')\n",
    "            \n",
    "            # Add current text/lang to tweet/language lists\n",
    "            self.texts.append(text)\n",
    "            self.langs.append(lang)\n",
    "        except: \n",
    "            pass\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print('ERROR: ', status)\n",
    "\n",
    "# Now let's get some data! \n",
    "# start with 10 results for testing. \n",
    "# once testing is done, increase to 10,000\n",
    "l = ListenerParser(max_results=10) \n",
    "\n",
    "auth = OAuthHandler(api_key, api_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "stream = Stream(auth, listener = l)\n",
    "stream.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check and make sure that you have the same number of texts as tweets. \n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "d = pd.read_csv('../my-data/tweets.csv')\n",
    "\n",
    "lang = []\n",
    "text = []\n",
    "for row in d.values:\n",
    "    if len(row) == 3:\n",
    "        if row[1] is not np.nan and row[2] is not np.nan:\n",
    "            lang.append(row[1])\n",
    "            text.append(row[2])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for l in lang:\n",
    "    if l is np.nan:\n",
    "        print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to use words to determine language. I think chars will have a higher rate of collision across mutliple languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.796005099873\n",
      "[[247   6   5   0   6   8   2   7   0   0   3   0]\n",
      " [ 18 322   4   2   8  10   4   0   0   0   2   2]\n",
      " [ 13  12 245   6  12  11   3   3   0   0   4   1]\n",
      " [  6   4  11 234   5   7   6   1   0   1   4   3]\n",
      " [  8   8   9   2 185   9   2   1   0   0   3   2]\n",
      " [  8   6  14   3   2 197   2   1   0   0   2   1]\n",
      " [  3   2   2   1   8   4 148   3   0   3   3   2]\n",
      " [  8   6   2   2   3   2   4 117   0   0   1   4]\n",
      " [  0   0   0   0   0   0   0   0   1   0   0   0]\n",
      " [  8   0   2   0   0   3  15   2   0  17   1   3]\n",
      " [  3  12  11   6   6   4   4   8   0   1  96   5]\n",
      " [  9  11   6   2   2   4   2   2   0   0   2  64]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         de       0.75      0.87      0.80       284\n",
      "         en       0.83      0.87      0.85       372\n",
      "         es       0.79      0.79      0.79       310\n",
      "         fr       0.91      0.83      0.87       282\n",
      "         it       0.78      0.81      0.79       229\n",
      "         pt       0.76      0.83      0.80       236\n",
      "         ru       0.77      0.83      0.80       179\n",
      "         sv       0.81      0.79      0.80       149\n",
      "         tl       1.00      1.00      1.00         1\n",
      "         uk       0.77      0.33      0.47        51\n",
      "        und       0.79      0.62      0.69       156\n",
      "         zh       0.74      0.62      0.67       104\n",
      "\n",
      "avg / total       0.80      0.80      0.79      2353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Next, some classifiers. \n",
    "# Start with logistic regression. \n",
    "# print a full classification report after you have trained the classifier and made predictions. \n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(text, lang)\n",
    "\n",
    "x_train = count_vect.fit_transform(x_train)\n",
    "x_test = count_vect.transform(x_test)\n",
    "\n",
    "\n",
    "model = LogisticRegression().fit(x_train, y_train)\n",
    "preds = model.predict(x_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y_test, preds))\n",
    "print(metrics.confusion_matrix(y_test, preds))\n",
    "print(metrics.classification_report(y_test, preds))\n",
    "\n",
    "##Not sure why i'm getting: ValueError: x and y must have same first dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB:\n",
      "0.765830854229\n",
      "[[240  19   2   0   8   8   2   4   0   0   1   0]\n",
      " [  6 344   5   0   6   6   2   1   0   0   2   0]\n",
      " [  1  30 256   4   7  12   0   0   0   0   0   0]\n",
      " [  2  19   9 235   6   6   2   1   0   0   1   1]\n",
      " [  7  13  17   2 175  11   1   2   0   0   1   0]\n",
      " [  2  17  17   4   2 191   1   0   0   0   1   1]\n",
      " [  2  13   3   5   8   5 139   4   0   0   0   0]\n",
      " [  7   9   3   3   4  11   1 110   0   0   1   0]\n",
      " [  0   1   0   0   0   0   0   0   0   0   0   0]\n",
      " [  3  10   1   0   3   7  23   0   0   3   1   0]\n",
      " [  2  24  13   9   6  22   6   1   0   0  72   1]\n",
      " [  7  32   6   2   1  11   5   0   0   0   3  37]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         de       0.86      0.85      0.85       284\n",
      "         en       0.65      0.92      0.76       372\n",
      "         es       0.77      0.83      0.80       310\n",
      "         fr       0.89      0.83      0.86       282\n",
      "         it       0.77      0.76      0.77       229\n",
      "         pt       0.66      0.81      0.73       236\n",
      "         ru       0.76      0.78      0.77       179\n",
      "         sv       0.89      0.74      0.81       149\n",
      "         tl       0.00      0.00      0.00         1\n",
      "         uk       1.00      0.06      0.11        51\n",
      "        und       0.87      0.46      0.60       156\n",
      "         zh       0.93      0.36      0.51       104\n",
      "\n",
      "avg / total       0.79      0.77      0.75      2353\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.4/site-packages/sklearn/metrics/metrics.py:1771: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Now do the same with multinomial naive bayes \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "print(\"MultinomialNB:\")\n",
    "nb_model = MultinomialNB().fit(x_train, y_train)\n",
    "preds = nb_model.predict(x_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y_test, preds))\n",
    "print(metrics.confusion_matrix(y_test, preds))\n",
    "print(metrics.classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Wow, that's some very high-dimensional data. \n",
    "# To estimate, if we had 150,000 dimensions * 10,000 datapoints * 4 bytes each (assuming we use int32 or float32; would be 8 byes if we are using int64 or float64)\n",
    "# the dense matrix would take up ~6 GB of memory. \n",
    "# \n",
    "# your laptop probably doesn't have that. \n",
    "# \n",
    "# To deal with this dimensionality problem, we have several options. Among them: \n",
    "# 1. Reduce dimensionality and convert to a dense array \n",
    "# -- use a dimensionality reduction algorithm (but we haven't covered that yet, so that's not really an option)\n",
    "# -- or set a max_features limit in scikit-learn's CountVectorizer\n",
    "# 2. Don't use this classifier \n",
    "# \n",
    "# For now, I'm going to go with 2. \n",
    "# My reasoning is: We already have several classifiers that are very good with high-dimensional data. \n",
    "# Even if we pared our data down to, say, 5000 dimensions using a max_features limit, \n",
    "# we would be throwing away a ton of information, but it would still be pretty tedious \n",
    "# to compute. \n",
    "# \n",
    "# You are free to choose whichever approach you want. \n",
    "# \n",
    "# But as a freebie, I'm going to choose a different classifier that does support sparse input \n",
    "# to use as a third option. This will be a sneak preview of support vector machines.\n",
    "# We will use sklearn's LinearSVC rather than SVC, because it is much faster to train (linear vs. quadratic or even cubic time)\n",
    "# and we don't need any of the extra functionality offered by a nonlinear svm that we could get using SVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.765830854229\n",
      "[[240  19   2   0   8   8   2   4   0   0   1   0]\n",
      " [  6 344   5   0   6   6   2   1   0   0   2   0]\n",
      " [  1  30 256   4   7  12   0   0   0   0   0   0]\n",
      " [  2  19   9 235   6   6   2   1   0   0   1   1]\n",
      " [  7  13  17   2 175  11   1   2   0   0   1   0]\n",
      " [  2  17  17   4   2 191   1   0   0   0   1   1]\n",
      " [  2  13   3   5   8   5 139   4   0   0   0   0]\n",
      " [  7   9   3   3   4  11   1 110   0   0   1   0]\n",
      " [  0   1   0   0   0   0   0   0   0   0   0   0]\n",
      " [  3  10   1   0   3   7  23   0   0   3   1   0]\n",
      " [  2  24  13   9   6  22   6   1   0   0  72   1]\n",
      " [  7  32   6   2   1  11   5   0   0   0   3  37]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         de       0.86      0.85      0.85       284\n",
      "         en       0.65      0.92      0.76       372\n",
      "         es       0.77      0.83      0.80       310\n",
      "         fr       0.89      0.83      0.86       282\n",
      "         it       0.77      0.76      0.77       229\n",
      "         pt       0.66      0.81      0.73       236\n",
      "         ru       0.76      0.78      0.77       179\n",
      "         sv       0.89      0.74      0.81       149\n",
      "         tl       0.00      0.00      0.00         1\n",
      "         uk       1.00      0.06      0.11        51\n",
      "        und       0.87      0.46      0.60       156\n",
      "         zh       0.93      0.36      0.51       104\n",
      "\n",
      "avg / total       0.79      0.77      0.75      2353\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.4/site-packages/sklearn/metrics/metrics.py:1771: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm_model = MultinomialNB().fit(x_train, y_train)\n",
    "preds = svm_model.predict(x_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y_test, preds))\n",
    "print(metrics.confusion_matrix(y_test, preds))\n",
    "print(metrics.classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# With a bit more preprocessing, this score could be substantially higher. \n",
    "# \n",
    "# Moving on, our data is less-than-ideal in a few ways: \n",
    "# 1. Several of the class labels occur only 1 or 2 times; \n",
    "# 2. There are only 10,000 datapoints\n",
    "# 3. We have done almost no preprocessing (of text, or of data after vectorization)\n",
    "# \n",
    "# From here on, this problem set is an open question: How can you improve on these base scores? \n",
    "# What's the best you can produce here? \n",
    "# \n",
    "# Go at it. And record your results in this notebook. \n",
    "# \n",
    "# Notes: \n",
    "# To determine your best score, use a cross validated score with 5 folds. \n",
    "# You are still only allowed to use the text. No meta-data! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
