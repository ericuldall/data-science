{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#GA LA Data Science 7 - Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For this homework, we will be using the library tweepy to pull data from the Twitter API, \n",
    "and then do language detection on it. That is, you will build a classifier to predict what language a given tweet is, based only on its text.\n",
    "\n",
    "Start by installing tweepy with pip: ```sudo pip install tweepy```\n",
    "\n",
    "Be sure to consult the documentation at http://tweepy.readthedocs.org/en/v3.3.0/index.html \n",
    "read early, read often. **NOTE:** Tweepy is among the most popular Twitter+Python libraries and is recommended by Twitter. However, it lacks complete documentation, e.g. the documentation is missing the Stream class.\n",
    "\n",
    "For the early parts, you can also look at this blog post: \n",
    "http://adilmoujahid.com/posts/2014/07/twitter-analytics/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In order to access Twitter Streaming API, we need to get 4 pieces of information from Twitter: \n",
    "    API key, \n",
    "    API secret, \n",
    "    Access token, and \n",
    "    Access token secret. \n",
    "Follow the steps below to get all 4 elements:\n",
    "\n",
    "- Create a twitter account if you do not already have one.\n",
    "- Go to https://apps.twitter.com/ and log in with your twitter credentials.\n",
    "- Click \"Create New App\"\n",
    "- Fill out the form, agree to the terms, and click \"Create your Twitter application\"\n",
    "- In the next page, click on \"Keys and Access Tokens\" tab, and copy your \"API Key\" and \"API Secret\".\n",
    "- Scroll down and click \"Create my access token\", and copy your \"Access Token\" and \"Access Token Secret\".\n",
    "- Double-check and ensure you copied the entire tokens rather than just part of them.\n",
    "\n",
    "Now that that's all set up, let's get get on with the fun stuff!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['en' 'es' 'fr' 'zh' 'sv' 'de' 'und' 'ru' 'tl' 'pt' 'uk' 'it']\n"
     ]
    }
   ],
   "source": [
    "# To start, let's just get a basic listener set up. All we want to do here is see \n",
    "# a stream of data from Twitter. \n",
    "# If this works, you should see a bunch of json's in your output console. \n",
    "# (If you see anything else -- such as 3-digit numbers or nothing at all -- then it's time to debug.)\n",
    "import requests\n",
    "import random\n",
    "import time\n",
    "import urllib\n",
    "import hmac\n",
    "from hashlib import sha1\n",
    "import base64\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Variables that contains the user credentials to access Twitter API \n",
    "api_key = b\"yGrCEinj1esitqUKnluIVRGFH\"\n",
    "api_secret = b\"a4loFJuuob0Ct63O97xxD4vwFqsj5FCTdkEOZsQIYY90FgruPM\"\n",
    "\n",
    "\n",
    "'''\n",
    "auth_token = api_key + b':' + api_secret\n",
    "get_b_token = base64.b64encode(auth_token).decode()\n",
    "p = requests.post('https://api.twitter.com/oauth2/token', headers={'Authorization': 'Basic ' + get_b_token, 'Content-Type': 'application/x-www-form-urlencoded;charset=UTF-8'}, data='grant_type=client_credentials')\n",
    "'''\n",
    "bearer_token = 'AAAAAAAAAAAAAAAAAAAAANUxgwAAAAAA03BiCesaCR%2B1WqSd6e6i5oDfI8M%3DV5wVG3HWNovQTSjim1QZRWAHSkV2OYX4cux3n15rltfgYZldUh'\n",
    "\n",
    "\n",
    "search_url = \"https://api.twitter.com/1.1/search/tweets.json\"\n",
    "search_query = {\n",
    "        \"q\": \"#paris\",\n",
    "        \"count\": \"100\",\n",
    "        \"result_type\": \"mixed\"\n",
    "}\n",
    "\n",
    "langs = ['au', 'eu', 'en', 'es', 'mx', 'fr', 'zh', 'tw', 'sv', 'su', 'de', 'ro', 'ru', 'sc', 'sa', 'pt', 'uk', 'it']\n",
    "tags  = ['yolo', 'vive', 'us', 'earth', 'planet', 'math', 'lang', 'hund', 'dog', 'france', 'da', 'peace', 'love', 'family']\n",
    "\n",
    "df = []\n",
    "\n",
    "for lang in langs:\n",
    "    for tag in tags:\n",
    "        sq = search_query.copy()\n",
    "        sq['q'] = '#' + tag\n",
    "        sq['lang'] = lang\n",
    "        url = search_url + '?' + urllib.parse.urlencode(sq)\n",
    "\n",
    "        g = requests.get(url, headers={'Authorization': 'Bearer ' + bearer_token})\n",
    "        statuses = g.json().get('statuses')\n",
    "        try:\n",
    "            for status in statuses:\n",
    "                df.append({'TEXT': status.get('text'), 'LANG': status.get('lang')})\n",
    "        except:\n",
    "            print('whoops... not iterable!')\n",
    "    \n",
    "\n",
    "d = pd.DataFrame(df)\n",
    "print(d['LANG'].unique())\n",
    "d.to_csv('./tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now that that's working, let's filter our data. \n",
    "# This part is actually pretty easy. \n",
    "# Change the on_data method of ListenerParser to extract only the 'text' and 'language' fields, if present.\n",
    "# We also want to be able to retrieve a set number of results, so I have set a max_results parameter \n",
    "# in the constructor. Use that in your edit of on_data to make the object only retrieve up to \n",
    "# max_results many results. \n",
    "\n",
    "# This is a listener that will extract the data we are interested in and print to stdout\n",
    "# Note this class is the same class as in the last example. Except now, in on_data we\n",
    "#    are processing the tweets instead of just printing them!\n",
    "class ListenerParser(StreamListener):\n",
    "    \n",
    "    def __init__(self, max_results): \n",
    "        super(ListenerParser, self).__init__()\n",
    "        \n",
    "        self.texts = []   # List of tweet messages\n",
    "        self.langs = []   # List of tweet languages\n",
    "        \n",
    "        if max_results:\n",
    "            self.max_results = max_results\n",
    "        else: \n",
    "            self.max_results = float(\"inf\")\n",
    "    \n",
    "    def on_data(self, data):\n",
    "        if len(l.texts) >= self.max_results:\n",
    "            return False   # stop collecting data\n",
    "        try:\n",
    "            # Converts current tweet from JSON to Python data structures\n",
    "            data = json.loads(data)\n",
    "            \n",
    "            # Look inside the 'data' variable, or read the Twitter docs\n",
    "            text = 'YOUR ANSWER HERE'   # Text of the tweet (from 'data')\n",
    "            lang = 'YOUR ANSWER HERE'   # Language of the tweet (from 'data')\n",
    "            \n",
    "            # Print status every 500 tweets loaded\n",
    "            if not (len(l.texts) % 500): \n",
    "                print(str(len(l.texts)) + ' tweets loaded!')\n",
    "            \n",
    "            # Add current text/lang to tweet/language lists\n",
    "            self.texts.append(text)\n",
    "            self.langs.append(lang)\n",
    "        except: \n",
    "            pass\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print('ERROR: ', status)\n",
    "\n",
    "# Now let's get some data! \n",
    "# start with 10 results for testing. \n",
    "# once testing is done, increase to 10,000\n",
    "l = ListenerParser(max_results=10) \n",
    "\n",
    "auth = OAuthHandler(api_key, api_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "stream = Stream(auth, listener = l)\n",
    "stream.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check and make sure that you have the same number of texts as tweets. \n",
    "import pandas as pd\n",
    "d = pd.read_csv('../my-data/tweets.csv')\n",
    "\n",
    "tr_df = []\n",
    "\n",
    "ts_df = []\n",
    "i = 0\n",
    "for row in d.values:\n",
    "    if i % 5 == 0:\n",
    "        ts_df.append({'LANG': row[1], 'TEXT': row[2]})\n",
    "    else:\n",
    "        tr_df.append({'LANG': row[1], 'TEXT': row[2]})\n",
    "        \n",
    "    i += 1\n",
    "    if i==100:\n",
    "        break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Awesome, now let's see if we can predict the language using only the text. \n",
    "# Use scikit-learn to split your data into train and test sets, \n",
    "# do the feature extraction on the text, \n",
    "# build two different classifiers: Logistic Regression and Naive Bayes,\n",
    "# and evaluate the results. \n",
    "\n",
    "# First, feature extraction. \n",
    "# Questions: do words or chars make more sense as features here? and what kind of ngram-range? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to use words to determine language. I think chars will have a higher rate of collision across mutliple languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   LANG                                               TEXT\n",
      "0    en  Another mental Saturday night. It will be near...\n",
      "1    en  Sometimes you take bedtime selfies w yer hat s...\n",
      "2    en  Currently just changed my entire outfit includ...\n",
      "3    en  I just like listening to @SpotifyAU's top 100 ...\n",
      "4    en             @CrankThatFrank so basically...\\n#YOLO\n",
      "5    en  YEMEN IN BLOOD AND FIRE\\nhttps://t.co/vmpXbb8G...\n",
      "6    en  @Braunger In good news, we built some truly FA...\n",
      "7    en  S W A G #withfilter #school #swag #cool #yolo ...\n",
      "8    en  RT @PanicCompanion: Hope &amp; Help For Your N...\n",
      "9    en  I show up early more often for baseball games ...\n",
      "10   en  Watching 30 Rock instead of getting ready for ...\n",
      "11   en      I used #yolo in a tweet. What is this 2012??😂\n",
      "12   en    Fucking quitters #yolo  https://t.co/vm9asL2bwZ\n",
      "13   en  Just got this email... #yolo #dontstopliving #...\n",
      "14   en  @Sarahandsnicker And I'm an hour from home. #YOLO\n",
      "15   en  RT @itskayladenise: my first time tryna put on...\n",
      "16   en  What #DeflateGate believers think is on Toms c...\n",
      "17   en  To Infinity &amp; Beyond  #Rodos #Lindos #Gree...\n",
      "18   en  Just posted a photo @ double posting because #...\n",
      "19   en  Just tweeting while beating @tompritchett52 at...\n",
      "20   en                               #YOLO go crazy!!!!!!\n",
      "21   en  WordPress –  Promote   Professional Stock Phot...\n",
      "22   en  Why @onebuckresume gets rated the top resume w...\n",
      "23   en               @Sashaspringmann frick yea bro #yolo\n",
      "24   en  I just ate 1000 calories worth of junk in 3hou...\n",
      "25   en  RT @RLump95: Man it's to hot outside to not go...\n",
      "26   en  Will do nasty things for a #oneplus2 phone inv...\n",
      "27   en  @Biff_Bruise but you're totally right about th...\n",
      "28   en  Do I look \"swaggin\" and cool yet??? #Deadpool ...\n",
      "29   en  Beach again? Yeah im getting black but ya know...\n",
      "..  ...                                                ...\n",
      "50   en  LOOK @GRAB_1558_FOLLOWS\\n\\n#dubai #best #wcw #...\n",
      "51   en  Breezy still pissed I wooped his ass plus took...\n",
      "52   en  RT @LeonorArroyo: Lets just treat each other t...\n",
      "53   en  Jus dropped a G on uwheels hoverboard pls call...\n",
      "54   en  \"If you ever get cold, just stand in the corne...\n",
      "55   en          Give me a legendary @MichaelCondrey #yolo\n",
      "56   en  #gaylove  #gay #happy  #instagay #gayboy #gayg...\n",
      "57   en  Brushed Other Grace's teeth post wisdom teeth ...\n",
      "58   en  Millenium force drunk might not go over too we...\n",
      "59   en  @vddfalcona because #yolo #swag #policiastem7b...\n",
      "60   en  Before and after. #haircut #menscut #mensfashi...\n",
      "61   en  Watching Nixon's 1968 telethon. Is that more o...\n",
      "62   en  Stonehenge live #diaverde #vegan #friends #bro...\n",
      "63   en  RT @ADayAtDenison: Selfie Time! #yolo #leaders...\n",
      "64   en  #YOLO so make it a daily priority to #LiveWell...\n",
      "65   en  Lets just treat each other the way we want to ...\n",
      "66   en  I'm just gonna not invite people places anymor...\n",
      "67   en      Only thing that cure watch addiction is #Yolo\n",
      "68   en  I’m starting to finally accept that grownups a...\n",
      "69   en  Welcome to the #jungle #l4l #f4f #like4like #n...\n",
      "70   en             Here is to an embarrassing night #yolo\n",
      "71   en  #yolo #swag #mlsallstar #MLS454 hotspur 4 - ML...\n",
      "72   en  Gggggoooooddmmmorrrnnniinnngg. I literally dri...\n",
      "73   en  Hanging out with two of frandz lol these guys ...\n",
      "74   en  RT @NeverSleepAlone: For those who didn't get ...\n",
      "75   en  Michell is trying to get me to not have a part...\n",
      "76   en      just emerged from 1200 feet underground #yolo\n",
      "77   en  Am I working? Yes. Am I debating taking a nap ...\n",
      "78   en  I hate when adults act like fucking swaggers a...\n",
      "79   en  Japanese classes start soon (about a week) and...\n",
      "\n",
      "[80 rows x 2 columns]\n",
      "   LANG                                               TEXT\n",
      "0    en  The #Yolo bailout: Greece's ex-finance chief h...\n",
      "1    en  don't feel like watching #ToughEnough since @H...\n",
      "2    en  RT @CasperHedin: This lad is going to spend $5...\n",
      "3    en  Anyone that says FroYo gets shot! #yolo  https...\n",
      "4    en  @Gl0bbinG0blin @GoldenBlackHawk just do it it'...\n",
      "5    en  Being peer pressured to go on a date with one ...\n",
      "6    en  It pisses me off that people try to tell me ho...\n",
      "7    en  im painting my nails at 2:30am #yolo #yoloswag...\n",
      "8    en              Bad decisions make good stories #yolo\n",
      "9    en  im not stupid. why on earth would i work while...\n",
      "10   en  RT @superultralicja: Me in my sexy dressing go...\n",
      "11   en  RT @slutlykpeppa: gorge jst wnked in mummy pig...\n",
      "12   en  I'm starting to get the feeling that I'm COMPL...\n",
      "13   en  RT @itskayladenise: my first time tryna put on...\n",
      "14   en  RT @LeonorArroyo: Lets just treat each other t...\n",
      "15   en            Now they back kool lol #thatright #YOLO\n",
      "16   en  Berryessa wildfire 2.0 update #california #dro...\n",
      "17   en  Living on Earth may be expensive, but it inclu...\n",
      "18   en                    #Windows10 is almost here #YOLO\n",
      "19   en  @lizardmess \"Life isn't hard to manage when yo...\n"
     ]
    }
   ],
   "source": [
    "# Next, split the data into train and test sets. \n",
    "tr_df = pd.DataFrame(tr_df)\n",
    "ts_df = pd.DataFrame(ts_df)\n",
    "tr_df.fillna('', inplace=True)\n",
    "ts_df.fillna('', inplace=True)\n",
    "print(tr_df) #training sets\n",
    "print(ts_df) #test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-142-0d9bc82a3a73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_ylim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'o'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/root/anaconda3/lib/python3.4/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   3097\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3098\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3099\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3100\u001b[0m         \u001b[0mdraw_if_interactive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3101\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda3/lib/python3.4/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1371\u001b[0m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1373\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1374\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1375\u001b[0m             \u001b[0mlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda3/lib/python3.4/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_grab_next_args\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    302\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 304\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mseg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    305\u001b[0m                     \u001b[1;32myield\u001b[0m \u001b[0mseg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda3/lib/python3.4/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs)\u001b[0m\n\u001b[0;32m    280\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'plot'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda3/lib/python3.4/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"x and y must have same first dimension\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"x and y can be no greater than 2-D\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension"
     ]
    },
    {
     "data": {
      "image/png": [
       "iVBORw0KGgoAAAANSUhEUgAAAs0AAAHfCAYAAACrueWMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\n",
       "AAALEgAACxIB0t1+/AAAFftJREFUeJzt3W+Iped53/Hf1ZVUMK2RjUBppDVK462xU+Q6JbIam3iM\n",
       "XbpVqVVCiVDsOLGdWpQqLX0T2SlYC2kCflEwxkUoQhaCgkWJTbMFIVW0HmyMolpgSXa962rjCFZy\n",
       "osbxH1LXL3bx1RdzlI6mu3uN5tk9s979fF7Nc577nPuGfZj5cu9zzqnuDgAAcHZ/Zb8XAAAAFzvR\n",
       "DAAAA9EMAAAD0QwAAAPRDAAAA9EMAACDxdFcVZ+uqher6qtnOf/eqnq6qp6pqi9V1Y1L5wQAgHU6\n",
       "HzvNDyQ5fI7z30zyC919Y5LfTvJ752FOAABYm8XR3N1fTPLdc5x/vLu/vzp8Isn1S+cEAIB1Wvc9\n",
       "zR9K8vCa5wQAgEWuWNdEVfXOJB9M8rYznPNd3gAArEV31yt9zlqiefXmv/uSHO7uM97KsZfFc2mr\n",
       "qiPdfWS/18HFxXXBmbguOBPXBWey183aC357RlW9Lsnnkryvu09c6PkAAOB8W7zTXFWfSfKOJNdU\n",
       "1ckkdye5Mkm6+94kH0vymiT3VFWSnOrum5bOCwAA67I4mrv79uH8ryf59aXzcFna3O8FcFHa3O8F\n",
       "cFHa3O8FcFHa3O8FcOmo7v1/D15VtXuaAQC40Pbanb5GGwAABqIZAAAGohkAAAaiGQAABqIZAAAG\n",
       "ohkAAAaiGQAABqIZAAAGohkAAAaiGQAABqIZAAAGohkAAAaiGQAABqIZAAAGohkAAAaiGQAABqIZ\n",
       "AAAGohkAAAaiGQAABqIZAAAGohkAAAaiGQAABqIZAAAGohkAAAaiGQAABqIZAAAGohkAAAaiGQAA\n",
       "BqIZAAAGohkAAAaiGQAABqIZAAAGohkAAAaiGQAABqIZAAAGohkAAAaiGQAABqIZAAAGohkAAAai\n",
       "GQAABqIZAAAGohkAAAaiGQAABqIZAAAGohkAAAaiGQAABqIZAAAGohkAAAaiGQAABqIZAAAGohkA\n",
       "AAaiGQAABqIZAAAGohkAAAaLormqPl1VL1bVV88x5pNV9WxVPV1Vb1kyHwAA7IelO80PJDl8tpNV\n",
       "dUuS13f3oSQfTnLPwvkAAGDtFkVzd38xyXfPMeQ9SR5cjX0iydVVde2SOQEAYN0u9D3N1yU5ue34\n",
       "+STXX+A5AQDgvLpiDXPUjuM+46CqI9sON7t780ItCACAy0NVbSTZWPo6FzqaX0hycNvx9avH/j/d\n",
       "feQCrwUAgMvMaiN286Xjqrp7L69zoW/POJrk/UlSVTcn+V53v3iB5wQAgPNq0U5zVX0myTuSXFNV\n",
       "J5PcneTKJOnue7v74aq6papOJPlBkg8sXTAAAKxbdZ/xFuP1LqKqu3vnvc8AAHBe7bU7fSMgAAAM\n",
       "RDMAAAxEMwAADEQzAAAMRDMAAAxEMwAADEQzAAAMRDMAAAxEMwAADEQzAAAMRDMAAAxEMwAADEQz\n",
       "AAAMRDMAAAxEMwAADEQzAAAMRDMAAAxEMwAADEQzAAAMRDMAAAxEMwAADEQzAAAMRDMAAAxEMwAA\n",
       "DEQzAAAMRDMAAAxEMwAADEQzAAAMRDMAAAxEMwAADEQzAAAMRDMAAAxEMwAADEQzAAAMRDMAAAxE\n",
       "MwAADEQzAAAMRDMAAAxEMwAADEQzAAAMRDMAAAxEMwAADEQzAAAMRDMAAAxEMwAADEQzAAAMRDMA\n",
       "AAxEMwAADEQzAAAMRDMAAAxEMwAADEQzAAAMRDMAAAxEMwAADEQzAAAMFkdzVR2uquNV9WxV3XWG\n",
       "89dU1SNV9VRVfa2qfm3pnAAAsE7V3Xt/ctWBJN9I8u4kLyT5cpLbu/vYtjFHkvzV7v5oVV2zGn9t\n",
       "d5/eNqa7u/a8EAAA2IW9dufSneabkpzo7ue6+1SSh5LcumPMnyR59ernVyf58+3BDAAAF7srFj7/\n",
       "uiQntx0/n+StO8bcl+S/VdW3kvz1JL+0cE4AAFirpdG8m3s7fivJU929UVU/neSxqnpzd//F9kGr\n",
       "2zhestndmwvXBgDAZa6qNpJsLH2dpdH8QpKD244PZmu3ebufT/I7SdLdf1RVf5zkDUme3D6ou48s\n",
       "XAsAALzMaiN286Xjqrp7L6+z9J7mJ5McqqobquqqJLclObpjzPFsvVEwVXVttoL5mwvnBQCAtVm0\n",
       "09zdp6vqziSPJjmQ5P7uPlZVd6zO35vkd5M8UFVPZyvSf7O7v7Nw3QAAsDaLPnLuvC3CR84BALAG\n",
       "+/WRcwAAcMkTzQAAMBDNAAAwEM0AADAQzQAAMBDNAAAwEM0AADAQzQAAMBDNAAAwEM0AADAQzQAA\n",
       "MBDNAAAwEM0AADAQzQAAMBDNAAAwEM0AADAQzQAAMBDNAAAwEM0AADAQzQAAMBDNAAAwEM0AADAQ\n",
       "zQAAMBDNAAAwEM0AADAQzQAAMBDNAAAwEM0AADAQzQAAMBDNAAAwEM0AADAQzQAAMBDNAAAwEM0A\n",
       "ADAQzQAAMBDNAAAwEM0AADAQzQAAMBDNAAAwEM0AADAQzQAAMBDNAAAwEM0AADAQzQAAMBDNAAAw\n",
       "EM0AADAQzQAAMBDNAAAwEM0AADAQzQAAMBDNAAAwEM0AADAQzQAAMBDNAAAwEM0AADBYHM1Vdbiq\n",
       "jlfVs1V111nGbFTVV6rqa1W1uXROAABYp+ruvT+56kCSbyR5d5IXknw5ye3dfWzbmKuTfCnJP+ju\n",
       "56vqmu7+9o7X6e6uPS8EAAB2Ya/duXSn+aYkJ7r7ue4+leShJLfuGPPLST7b3c8nyc5gBgCAi93S\n",
       "aL4uycltx8+vHtvuUJLXVtXnq+rJqvqVhXMCAMBaXbHw+bu5t+PKJD+b5F1JXpXk8ar6w+5+dvug\n",
       "qjqy7XCzuzcXrg0AgMtcVW0k2Vj6Okuj+YUkB7cdH8zWbvN2J5N8u7t/mOSHVfWFJG9O8rJo7u4j\n",
       "C9cCAAAvs9qI3XzpuKru3svrLL0948kkh6rqhqq6KsltSY7uGPMHSd5eVQeq6lVJ3prk6wvnBQCA\n",
       "tVm009zdp6vqziSPJjmQ5P7uPlZVd6zO39vdx6vqkSTPJPlRkvu6WzQDAPBjY9FHzp23RfjIOQAA\n",
       "1mC/PnIOAAAueaIZAAAGohkAAAaiGQAABqIZAAAGohkAAAaiGQAABqIZAAAGohkAAAaiGQAABqIZ\n",
       "AAAGohkAAAaiGQAABqIZAAAGohkAAAaiGQAABqIZAAAGohkAAAaiGQAABqIZAAAGohkAAAaiGQAA\n",
       "BqIZAAAGohkAAAaiGQAABqIZAAAGohkAAAaiGQAABqIZAAAGohkAAAaiGQAABqIZAAAGohkAAAai\n",
       "GQAABqIZAAAGohkAAAaiGQAABqIZAAAGohkAAAaiGQAABqIZAAAGohkAAAaiGQAABqIZAAAGohkA\n",
       "AAaiGQAABqIZAAAGohkAAAaiGQAABqIZAAAGohkAAAaiGQAABqIZAAAGohkAAAaLo7mqDlfV8ap6\n",
       "tqruOse4n6uq01X1i0vnBACAdVoUzVV1IMmnkhxO8qYkt1fVG88y7uNJHklSS+YEAIB1W7rTfFOS\n",
       "E939XHefSvJQklvPMO43kvx+kj9bOB8AAKzd0mi+LsnJbcfPrx77S1V1XbZC+p7VQ71wTgAAWKsr\n",
       "Fj5/NwH8iSQf6e6uqspZbs+oqiPbDje7e3Ph2gAAuMxV1UaSjcWv0733jd+qujnJke4+vDr+aJIf\n",
       "dffHt435Zv5fKF+T5P8k+WfdfXTbmO5u9zoDAHBB7bU7l0bzFUm+keRdSb6V5L8nub27j51l/ANJ\n",
       "/nN3f27H46IZAIALbq/duej2jO4+XVV3Jnk0yYEk93f3saq6Y3X+3iWvDwAAF4NFO83nbRF2mgEA\n",
       "WIO9dqdvBAQAgIFoBgCAgWgGAICBaAYAgIFoBgCAgWgGAICBaAYAgIFoBgCAgWgGAICBaAYAgIFo\n",
       "BgCAgWgGAICBaAYAgIFoBgCAgWgGAICBaAYAgIFoBgCAgWgGAICBaAYAgIFoBgCAgWgGAICBaAYA\n",
       "gIFoBgCAgWgGAICBaAYAgIFoBgCAgWgGAICBaAYAgIFoBgCAgWgGAICBaAYAgIFoBgCAgWgGAICB\n",
       "aAYAgIFoBgCAgWgGAICBaAYAgIFoBgCAgWgGAICBaAYAgIFoBgCAgWgGAICBaAYAgIFoBgCAgWgG\n",
       "AICBaAYAgIFoBgCAgWgGAICBaAYAgIFoBgCAgWgGAICBaAYAgIFoBgCAgWgGAICBaAYAgMHiaK6q\n",
       "w1V1vKqeraq7znD+vVX1dFU9U1Vfqqobl84JAADrVN299ydXHUjyjSTvTvJCki8nub27j20b8/eS\n",
       "fL27v19Vh5Mc6e6bd7xOd3fteSEAALALe+3OpTvNNyU50d3PdfepJA8luXX7gO5+vLu/vzp8Isn1\n",
       "C+cEAIC1WhrN1yU5ue34+dVjZ/OhJA8vnBMAANbqioXP3/W9HVX1ziQfTPK2s5w/su1ws7s3F60M\n",
       "AIDLXlVtJNlY+jpLo/mFJAe3HR/M1m7zy6ze/HdfksPd/d0zvVB3H1m4FgAAeJnVRuzmS8dVdfde\n",
       "Xmfp7RlPJjlUVTdU1VVJbktydPuAqnpdks8leV93n1g4HwAArN2inebuPl1VdyZ5NMmBJPd397Gq\n",
       "umN1/t4kH0vymiT3VFWSnOrum5YtGwAA1mfRR86dt0X4yDkAANZgvz5yDgAALnmiGQAABqIZAAAG\n",
       "ohkAAAaiGQAABqIZAAAGohkAAAaiGQAABqIZAAAGohkAAAaiGQAABqIZAAAGohkAAAaiGQAABqIZ\n",
       "AAAGohkAAAaiGQAABqIZAAAGohkAAAaiGQAABqIZAAAGohkAAAaiGQAABqIZAAAGohkAAAaiGQAA\n",
       "BqIZAAAGohkAAAaiGQAABqIZAAAGohkAAAaiGQAABqIZAAAGohkAAAaiGQAABqIZAAAGohkAAAai\n",
       "GQAABqIZAAAGohkAAAaiGQAABqIZAAAGohkAAAaiGQAABqIZAAAGohkAAAaiGQAABqIZAAAGohkA\n",
       "AAaiGQAABqIZAAAGohkAAAaiGQAABqIZAAAGohkAAAaLo7mqDlfV8ap6tqruOsuYT67OP11Vb1k6\n",
       "JwAArNOiaK6qA0k+leRwkjclub2q3rhjzC1JXt/dh5J8OMk9S+YEAIB1W7rTfFOSE939XHefSvJQ\n",
       "klt3jHlPkgeTpLufSHJ1VV27cF4AAFibpdF8XZKT246fXz02jbl+4bwAALA2Vyx8fu9yXE3Pq6oj\n",
       "2w43u3tzj2sCAIAkSVVtJNlY+jpLo/mFJAe3HR/M1k7yucZcv3rsZbr7yMK1AADAy6w2YjdfOq6q\n",
       "u/fyOktvz3gyyaGquqGqrkpyW5KjO8YcTfL+JKmqm5N8r7tfXDgvAACszaKd5u4+XVV3Jnk0yYEk\n",
       "93f3saq6Y3X+3u5+uKpuqaoTSX6Q5AOLVw0AAGtU3bu9LfkCLqKqu3vnfc8AAHBe7bU7fSMgAAAM\n",
       "RDMAAAxEMwAADEQzAAAMRDMAAAxEMwAADEQzAAAMRDMAAAxEMwAADEQzAAAMRDMAAAxEMwAADEQz\n",
       "AAAMRDMAAAxEMwAADEQzAAAMRDMAAAxEMwAADEQzAAAMRDMAAAxEMwAADEQzAAAMRDMAAAxEMwAA\n",
       "DEQzAAAMRDMAAAxEMwAADEQzAAAMRDMAAAxEMwAADEQzAAAMRDMAAAxEMwAADEQzAAAMRDMAAAxE\n",
       "MwAADEQzAAAMRDMAAAxEMwAADEQzAAAMRDMAAAxEMwAADEQzAAAMRDMAAAxEMwAADEQzAAAMRDMA\n",
       "AAxEMwAADEQzAAAMRDMAAAxEMwAADEQzAAAMRDMAAAxEMwAADEQzAAAM9hzNVfXaqnqsqv5nVf2X\n",
       "qrr6DGMOVtXnq+p/VNXXqupfLlsul5Oq2tjvNXDxcV1wJq4LzsR1wfm0ZKf5I0ke6+6/leS/ro53\n",
       "OpXkX3f3zyS5Ocm/qKo3LpiTy8vGfi+Ai9LGfi+Ai9LGfi+Ai9LGfi+AS8eSaH5PkgdXPz+Y5J/s\n",
       "HNDdf9rdT61+/t9JjiX5yQVzAgDA2i2J5mu7+8XVzy8mufZcg6vqhiRvSfLEgjkBAGDtqrvPfrLq\n",
       "sSQ/cYZT/ybJg939mm1jv9Pdrz3L6/y1JJtJ/m13/6cznD/7IgAA4Dzq7nqlz7lieMG/f7ZzVfVi\n",
       "Vf1Ed/9pVf2NJP/rLOOuTPLZJP/hTMG8mucVLxwAANZlye0ZR5P86urnX01yph3kSnJ/kq939ycW\n",
       "zAUAAPvmnLdnnPOJVa9N8h+TvC7Jc0l+qbu/V1U/meS+7v5HVfX2JF9I8kySlyb6aHc/snjlAACw\n",
       "JnuOZgAAuFys9RsBq+pwVR2vqmer6q6zjPnk6vzTVfWWda6P/TFdF1X13tX18ExVfamqbtyPdbJe\n",
       "u/l9sRr3c1V1uqp+cZ3rY3/s8u/IRlV9ZfWlWptrXiL7YBd/R66pqkeq6qnVdfFr+7BM1qiqPr16\n",
       "/91XzzHmFTXn2qK5qg4k+VSSw0nelOT2nV90UlW3JHl9dx9K8uEk96xrfeyP3VwXSb6Z5Be6+8Yk\n",
       "v53k99a7StZtl9fFS+M+nuSRJN5QfInb5d+Rq5P8+yT/uLv/dpJ/uvaFsla7/H1xZ5KvdPffydYX\n",
       "nvy7qjrnhyHwY++BbF0TZ7SX5lznTvNNSU5093PdfSrJQ0lu3THmL78wpbufSHJ1VZ3z85/5sTde\n",
       "F939eHd/f3X4RJLr17xG1m83vy+S5DeS/H6SP1vn4tg3u7kufjnJZ7v7+STp7m+veY2s326uiz9J\n",
       "8urVz69O8ufdfXqNa2TNuvuLSb57jiGvuDnXGc3XJTm57fj51WPTGIF0advNdbHdh5I8fEFXxMVg\n",
       "vC6q6rps/WF8aXfAGzQufbv5fXEoyWur6vNV9WRV/craVsd+2c11cV+Sn6mqbyV5Osm/WtPauHi9\n",
       "4uZc539N7PYP2s7/YvWH8NK263/fqnpnkg8meduFWw4Xid1cF59I8pHu7tXHW7o949K3m+viyiQ/\n",
       "m+RdSV6V5PGq+sPufvaCroz9tJvr4reSPNXdG1X100keq6o3d/dfXOC1cXF7Rc25zmh+IcnBbccH\n",
       "s1X15xpz/eoxLl27uS6yevPffUkOd/e5/ruFS8Nurou/m+ShrV7ONUn+YVWd6u6j61ki+2A318XJ\n",
       "JN/u7h8m+WFVfSHJm5OI5kvXbq6Ln0/yO0nS3X9UVX+c5A1JnlzLCrkYveLmXOftGU8mOVRVN1TV\n",
       "VUluy9YXpGx3NMn7k6Sqbk7yve5+cY1rZP3G66KqXpfkc0ne190n9mGNrN94XXT33+zun+run8rW\n",
       "fc3/XDBf8nbzd+QPkry9qg5U1auSvDXJ19e8TtZrN9fF8STvTpLVfatvyNabzLl8veLmXNtOc3ef\n",
       "rqo7kzya5ECS+7v7WFXdsTp/b3c/XFW3VNWJJD9I8oF1rY/9sZvrIsnHkrwmyT2rXcVT3X3Tfq2Z\n",
       "C2+X1wWXmV3+HTleVY9k60u1fpStL9sSzZewXf6++N0kD1TV09naMPzN7v7Ovi2aC66qPpPkHUmu\n",
       "qaqTSe7O1u1be25OX24CAACDtX65CQAA/DgSzQAAMBDNAAAwEM0AADAQzQAAMBDNAAAwEM0AADD4\n",
       "v6a609XOqTi+AAAAAElFTkSuQmCC\n"
      ],
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3ddbe6c6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Next, some classifiers. \n",
    "# Start with logistic regression. \n",
    "# print a full classification report after you have trained the classifier and made predictions. \n",
    "\n",
    "lang_map = {'au': 0.0, 'eu': 0.1, 'en': 0.2, 'es': 0.3, 'mx': 0.4, 'fr': 0.5, 'zh': 0.6, 'tw': 0.7, 'sv': 0.8, 'su': 0.9, 'de': 0.10, \n",
    "            'ro': 0.11, 'ru': 0.12, 'sc': 0.13, 'sa': 0.14, 'pt': 0.15, 'uk': 0.16, 'it': 0.17}\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(ngram_range=(1, 2))\n",
    "x = count_vect.fit_transform(tr_df.TEXT)\n",
    "\n",
    "tr_df['LANG_KEY'] = tr_df['LANG'].map(lang_map)\n",
    "y = tr_df['LANG_KEY']\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y)\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "fig, axes = plt.subplots(1,1, figsize=(12, 8))\n",
    "axes.set_ylim((-0.2, 1.2))\n",
    "plt.plot(xtrain, ytrain, 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: array([ 0.15,  0.12,  0.1 , ...,  0.17,  0.2 ,  0.2 ])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-130-7c7637ebcd99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"MultinomialNB:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan_to_num\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan_to_num\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0maccuracy_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda3/lib/python3.4/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m         \u001b[0mlabelbin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 305\u001b[1;33m         \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabelbin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabelbin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda3/lib/python3.4/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    424\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    427\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda3/lib/python3.4/site-packages/sklearn/preprocessing/label.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    304\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_type_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse_input_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 306\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda3/lib/python3.4/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36munique_labels\u001b[1;34m(*ys)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[0m_unique_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_FN_UNIQUE_LABELS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_unique_labels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unknown label type: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[0mys_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_unique_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown label type: array([ 0.15,  0.12,  0.1 , ...,  0.17,  0.2 ,  0.2 ])"
     ]
    }
   ],
   "source": [
    "# Now do the same with multinomial naive bayes \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "print(\"MultinomialNB:\")\n",
    "clf = MultinomialNB().fit(np.nan_to_num(xtrain), np.nan_to_num(ytrain))\n",
    "accuracy_report(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Wow, that's some very high-dimensional data. \n",
    "# To estimate, if we had 150,000 dimensions * 10,000 datapoints * 4 bytes each (assuming we use int32 or float32; would be 8 byes if we are using int64 or float64)\n",
    "# the dense matrix would take up ~6 GB of memory. \n",
    "# \n",
    "# your laptop probably doesn't have that. \n",
    "# \n",
    "# To deal with this dimensionality problem, we have several options. Among them: \n",
    "# 1. Reduce dimensionality and convert to a dense array \n",
    "# -- use a dimensionality reduction algorithm (but we haven't covered that yet, so that's not really an option)\n",
    "# -- or set a max_features limit in scikit-learn's CountVectorizer\n",
    "# 2. Don't use this classifier \n",
    "# \n",
    "# For now, I'm going to go with 2. \n",
    "# My reasoning is: We already have several classifiers that are very good with high-dimensional data. \n",
    "# Even if we pared our data down to, say, 5000 dimensions using a max_features limit, \n",
    "# we would be throwing away a ton of information, but it would still be pretty tedious \n",
    "# to compute. \n",
    "# \n",
    "# You are free to choose whichever approach you want. \n",
    "# \n",
    "# But as a freebie, I'm going to choose a different classifier that does support sparse input \n",
    "# to use as a third option. This will be a sneak preview of support vector machines.\n",
    "# We will use sklearn's LinearSVC rather than SVC, because it is much faster to train (linear vs. quadratic or even cubic time)\n",
    "# and we don't need any of the extra functionality offered by a nonlinear svm that we could get using SVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'YOUR ANSWER HERE'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\"\"\"YOUR ANSWER HERE\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# With a bit more preprocessing, this score could be substantially higher. \n",
    "# \n",
    "# Moving on, our data is less-than-ideal in a few ways: \n",
    "# 1. Several of the class labels occur only 1 or 2 times; \n",
    "# 2. There are only 10,000 datapoints\n",
    "# 3. We have done almost no preprocessing (of text, or of data after vectorization)\n",
    "# \n",
    "# From here on, this problem set is an open question: How can you improve on these base scores? \n",
    "# What's the best you can produce here? \n",
    "# \n",
    "# Go at it. And record your results in this notebook. \n",
    "# \n",
    "# Notes: \n",
    "# To determine your best score, use a cross validated score with 5 folds. \n",
    "# You are still only allowed to use the text. No meta-data! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
